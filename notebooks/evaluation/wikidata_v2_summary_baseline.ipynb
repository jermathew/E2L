{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:31.542795Z",
     "start_time": "2020-12-18T14:43:31.538003Z"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '../../'\n",
    "src_dir = 'src'\n",
    "data_dir = 'data/corpus'\n",
    "models_dir = 'data/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:31.550073Z",
     "start_time": "2020-12-18T14:43:31.546520Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:31.557594Z",
     "start_time": "2020-12-18T14:43:31.553502Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(root_dir, src_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:31.564352Z",
     "start_time": "2020-12-18T14:43:31.560763Z"
    }
   },
   "outputs": [],
   "source": [
    "version = 'v2'\n",
    "corpus_filename = f'wikidata_corpus_{version}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:32.598685Z",
     "start_time": "2020-12-18T14:43:31.567220Z"
    }
   },
   "outputs": [],
   "source": [
    "from training import TrainingCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:32.618576Z",
     "start_time": "2020-12-18T14:43:32.600067Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = TrainingCorpus()\n",
    "corpus.load(os.path.join(root_dir, data_dir, corpus_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:32.627147Z",
     "start_time": "2020-12-18T14:43:32.620435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3294"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builds pseudo-docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:32.630757Z",
     "start_time": "2020-12-18T14:43:32.628659Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:32.634208Z",
     "start_time": "2020-12-18T14:43:32.632071Z"
    }
   },
   "outputs": [],
   "source": [
    "pseudodocs_dict = defaultdict(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:32.640707Z",
     "start_time": "2020-12-18T14:43:32.635598Z"
    }
   },
   "outputs": [],
   "source": [
    "for doc_id in corpus.docs:\n",
    "    text = corpus.get_text(doc_id)\n",
    "    label = corpus.target[doc_id][0]\n",
    "    pseudodocs_dict[label] += ' ' + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T09:34:52.385404Z",
     "start_time": "2020-12-03T09:34:52.380486Z"
    }
   },
   "source": [
    "## Summarize pseudo-docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:32.644479Z",
     "start_time": "2020-12-18T14:43:32.641982Z"
    }
   },
   "outputs": [],
   "source": [
    "entities = list(pseudodocs_dict.keys())\n",
    "num_entities = len(entities)\n",
    "pseudodocs = [pseudodocs_dict[e_id] for e_id in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:32.648062Z",
     "start_time": "2020-12-18T14:43:32.645982Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'facebook/bart-large-cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:46.222961Z",
     "start_time": "2020-12-18T14:43:32.649663Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T14:43:46.229434Z",
     "start_time": "2020-12-18T14:43:46.225856Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "do_sample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:23.216538Z",
     "start_time": "2020-12-18T14:43:46.232199Z"
    }
   },
   "outputs": [],
   "source": [
    "summaries = []\n",
    "\n",
    "for i in range(num_entities):\n",
    "    doc = pseudodocs[i]\n",
    "    encoded_doc = tokenizer([doc], padding=True, truncation=True, return_tensors='pt')\n",
    "    summary_ids = model.generate(encoded_doc['input_ids'], max_length=max_len, do_sample=do_sample)\n",
    "    summary_text = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0]\n",
    "    summaries.append(summary_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save summaries to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:23.232300Z",
     "start_time": "2020-12-18T15:02:23.218968Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:23.253341Z",
     "start_time": "2020-12-18T15:02:23.236094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1754</td>\n",
       "      <td>Stockholm is the capital and most populous ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1787199</td>\n",
       "      <td>Stockholm is a town in Grant County, South Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q976601</td>\n",
       "      <td>Stockholm is a village in Pepin County, Wisco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1484620</td>\n",
       "      <td>Stockholm asteroid  10552 Stockholm asteroid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q3447382</td>\n",
       "      <td>Stockholm is a town in Aroostook County, Main...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     entity                                            summary\n",
       "0     Q1754   Stockholm is the capital and most populous ur...\n",
       "1  Q1787199   Stockholm is a town in Grant County, South Da...\n",
       "2   Q976601   Stockholm is a village in Pepin County, Wisco...\n",
       "3  Q1484620   Stockholm asteroid  10552 Stockholm asteroid ...\n",
       "4  Q3447382   Stockholm is a town in Aroostook County, Main..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_summaries_df = pd.DataFrame([(e, s) for e, s in zip(entities, summaries)], columns=['entity', 'summary'])\n",
    "raw_summaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:23.453039Z",
     "start_time": "2020-12-18T15:02:23.255298Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_summaries_filename = f'raw_summary_wikidata_{version}.xlsx'\n",
    "raw_summaries_filepath = os.path.join(root_dir, 'data/terms', raw_summaries_filename)\n",
    "raw_summaries_df.to_excel(raw_summaries_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Compute ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:23.464544Z",
     "start_time": "2020-12-18T15:02:23.454468Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_mix(seq, subseq):\n",
    "    n = len(seq)\n",
    "    m = len(subseq)\n",
    "    for i in range(n - m + 1):\n",
    "        if seq[i] == subseq[0] and seq[i:i + m] == subseq:\n",
    "            yield range(i, i + m)\n",
    "            \n",
    "\n",
    "def get_chunk_document(chunks, text) -> list:\n",
    "    tokens = TrainingCorpus.tokenize(text.lower())\n",
    "    \n",
    "    if len(chunks) > 0:\n",
    "        for k_chunk in chunks:\n",
    "            chunk = k_chunk.split('_')\n",
    "            replacements = [r for r in find_mix(tokens, chunk)]\n",
    "            l, f = 0, []\n",
    "            \n",
    "            while l < len(tokens):\n",
    "                replaced = False\n",
    "                \n",
    "                for r in replacements:\n",
    "                    if l in r:\n",
    "                        replaced = True\n",
    "                        f.append(chunk)\n",
    "                        l += len(chunk)\n",
    "                        break\n",
    "                    else:\n",
    "                        pass\n",
    "                if not replaced:\n",
    "                    f.append(tokens[l])\n",
    "                    l += 1\n",
    "            \n",
    "            new_tokens = []\n",
    "            \n",
    "            for x in f:\n",
    "                if isinstance(x, list):\n",
    "                    new_tokens.append(\"_\".join(x))\n",
    "                else:\n",
    "                    new_tokens.append(x)\n",
    "            tokens = new_tokens\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get chunk document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:23.468384Z",
     "start_time": "2020-12-18T15:02:23.466066Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:26.999339Z",
     "start_time": "2020-12-18T15:02:23.470029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7cd85629314543b13ddad875805f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=195.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "summary_chunks = []\n",
    "spacy_model_name = 'en_core_web_sm'\n",
    "spacy_model = spacy.load(spacy_model_name)\n",
    "\n",
    "for s in tqdm(summaries):\n",
    "    chunks = []\n",
    "    for chunk in spacy_model(s.lower()).noun_chunks:\n",
    "        c = '_'.join(TrainingCorpus.tokenize(chunk.text))\n",
    "        if c:\n",
    "            chunks.append(c)\n",
    "    \n",
    "    chunk_doc = get_chunk_document(chunks, s)\n",
    "    summary_chunks.append(chunk_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:27.004754Z",
     "start_time": "2020-12-18T15:02:27.001056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tangerang city province banten indonesia latest_official_estimate mid 2020 2,273,697 – making eighth populated suburb world latter_date area 164_54_square_kilometres 63_53_square_miles'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chunks[44]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T17:22:53.296297Z",
     "start_time": "2020-12-08T17:22:53.291752Z"
    }
   },
   "source": [
    "### Save chunks to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:27.014709Z",
     "start_time": "2020-12-18T15:02:27.006362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1754</td>\n",
       "      <td>stockholm capital populous_urban_area sweden w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1787199</td>\n",
       "      <td>stockholm_town grant_county south_dakota unite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q976601</td>\n",
       "      <td>stockholm village pepin_county wisconsin unite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1484620</td>\n",
       "      <td>stockholm asteroid 10552_stockholm_asteroid as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q3447382</td>\n",
       "      <td>stockholm town aroostook_county maine united_s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     entity                                            summary\n",
       "0     Q1754  stockholm capital populous_urban_area sweden w...\n",
       "1  Q1787199  stockholm_town grant_county south_dakota unite...\n",
       "2   Q976601  stockholm village pepin_county wisconsin unite...\n",
       "3  Q1484620  stockholm asteroid 10552_stockholm_asteroid as...\n",
       "4  Q3447382  stockholm town aroostook_county maine united_s..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_df = pd.DataFrame([(e, s) for e, s in zip(entities, summary_chunks)], columns=['entity', 'summary'])\n",
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:27.036437Z",
     "start_time": "2020-12-18T15:02:27.016153Z"
    }
   },
   "outputs": [],
   "source": [
    "chunks_filename = f'chunk_summary_wikidata_{version}.xlsx'\n",
    "chunks_filepath = os.path.join(root_dir, 'data/terms', chunks_filename)\n",
    "chunks_df.to_excel(chunks_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute tf-idf on each pseudo-document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:27.411497Z",
     "start_time": "2020-12-18T15:02:27.037881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pseudodocs = []\n",
    "\n",
    "for i in range(num_entities):\n",
    "    doc = pseudodocs[i].lower()\n",
    "    clean_doc = ' '.join(TrainingCorpus.tokenize(doc))\n",
    "    clean_pseudodocs.append(clean_doc)\n",
    "\n",
    "len(clean_pseudodocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T11:18:53.469356Z",
     "start_time": "2020-12-05T11:18:53.464468Z"
    }
   },
   "source": [
    "# Fit tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:27.415325Z",
     "start_time": "2020-12-18T15:02:27.412925Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:27.442644Z",
     "start_time": "2020-12-18T15:02:27.416963Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=False, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function <lambda> at 0x7feafcbeb9d0>, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=False, tokenizer=lambda x: x.split())\n",
    "vectorizer.fit(clean_pseudodocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save tf-idf weights into a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T15:02:27.462106Z",
     "start_time": "2020-12-18T15:02:27.443876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 4444)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_docs = vectorizer.transform(clean_pseudodocs)\n",
    "vectorized_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-18T14:43:31.488Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_weights = []\n",
    "\n",
    "for i in range(num_entities):\n",
    "    doc_vector = vectorized_docs[i].toarray().reshape(-1)\n",
    "    weights = {}\n",
    "    \n",
    "    for j, w in enumerate(doc_vector):\n",
    "        feature_name = vectorizer.get_feature_names()[j]\n",
    "        if w > 0:\n",
    "            weights[feature_name] = w\n",
    "    \n",
    "    tfidf_weights.append(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute baseline summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-18T14:43:31.491Z"
    }
   },
   "outputs": [],
   "source": [
    "min_len = 1\n",
    "max_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-18T14:43:31.493Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_score(idx, text):\n",
    "    if text:\n",
    "        weights = tfidf_weights[idx]\n",
    "        tokenized_text = TrainingCorpus.tokenize(text.lower())\n",
    "        score = 0\n",
    "\n",
    "        if tokenized_text:\n",
    "            for token in tokenized_text:\n",
    "                if token in weights:\n",
    "                    score += weights[token]\n",
    "        text_len = len(text.split())\n",
    "        score = score/text_len\n",
    "        return score\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-18T14:43:31.495Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-18T14:43:31.497Z"
    }
   },
   "outputs": [],
   "source": [
    "final_summaries = []\n",
    "\n",
    "for i in range(num_entities):\n",
    "    # split on sentences\n",
    "    final_summary = []\n",
    "    sentences = nltk.tokenize.sent_tokenize(summaries[i])\n",
    "    prev_score = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        score = compute_score(i, sentence)\n",
    "        delta_score = score - prev_score\n",
    "        \n",
    "        if delta_score > 0:\n",
    "            final_summary.append(sentence)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        prev_score = score\n",
    "    \n",
    "    final_summaries.append(' '.join(final_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a DataFrame out of summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-18T14:43:31.500Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([(e, s) for e, s in zip(entities, final_summaries)], columns=['entity', 'summary'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DataFrame to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-18T14:43:31.502Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = f'summary_baseline_wikidata_{version}.xlsx'\n",
    "filepath = os.path.join(root_dir, 'data/terms', filename)\n",
    "df.to_excel(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
