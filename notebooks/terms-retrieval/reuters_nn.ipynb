{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find cluster related terms using LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:53.938769Z",
     "start_time": "2020-09-28T16:37:53.934073Z"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '../..'\n",
    "data_dir = 'data'\n",
    "corpus_dir = 'corpus'\n",
    "src_dir = 'src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:54.127434Z",
     "start_time": "2020-09-28T16:37:54.124140Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:54.311818Z",
     "start_time": "2020-09-28T16:37:54.307735Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(root_dir, src_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:54.495514Z",
     "start_time": "2020-09-28T16:37:54.491578Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_filename = 'reuters_corpus.json'\n",
    "corpus_filepath = os.path.join(root_dir, data_dir, corpus_dir, corpus_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:54.669003Z",
     "start_time": "2020-09-28T16:37:54.665158Z"
    }
   },
   "outputs": [],
   "source": [
    "chunks_filename = 'reuters_chunks.json'\n",
    "chunks_filepath = os.path.join(root_dir, data_dir, corpus_dir, chunks_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:56.015520Z",
     "start_time": "2020-09-28T16:37:55.068894Z"
    }
   },
   "outputs": [],
   "source": [
    "from training import TrainingCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:56.682462Z",
     "start_time": "2020-09-28T16:37:56.017626Z"
    }
   },
   "outputs": [],
   "source": [
    "reuters_corpus = TrainingCorpus()\n",
    "reuters_corpus.load(corpus_filepath)\n",
    "reuters_corpus.load_chunks(chunks_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:58.066871Z",
     "start_time": "2020-09-28T16:37:56.685071Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import TensorflowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:58.071288Z",
     "start_time": "2020-09-28T16:37:58.068814Z"
    }
   },
   "outputs": [],
   "source": [
    "models_dir = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:58.076595Z",
     "start_time": "2020-09-28T16:37:58.072597Z"
    }
   },
   "outputs": [],
   "source": [
    "model_filename = 'reuters_nn_model.h5'\n",
    "model_filepath = os.path.join(root_dir, data_dir, models_dir, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:37:58.080857Z",
     "start_time": "2020-09-28T16:37:58.078357Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_filename = 'reuters_nn_word_index.csv'\n",
    "word_index_filepath = os.path.join(root_dir, data_dir, models_dir, word_index_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:01.638855Z",
     "start_time": "2020-09-28T16:37:58.082363Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nn_model = TensorflowModel(model_filepath, word_index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sample data\n",
    "### Load the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:03.862890Z",
     "start_time": "2020-09-28T16:38:03.858017Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:06.817259Z",
     "start_time": "2020-09-28T16:38:04.334656Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_model = load_model(model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to load a subset of `tf_model` such that the last layer is the LSTM layer. Using this neural network we can get for each input its corresponding embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:06.824178Z",
     "start_time": "2020-09-28T16:38:06.819230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index\tLayer\n",
      "-------------------------\n",
      "0 \t input_1\n",
      "1 \t embedding_1\n",
      "2 \t bidirectional_1\n",
      "3 \t dense_1\n",
      "4 \t dense_2\n"
     ]
    }
   ],
   "source": [
    "print('Index\\tLayer')\n",
    "print('-------------------------')\n",
    "for index, layer in enumerate(tf_model.layers):\n",
    "    print(index,'\\t',layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:06.829829Z",
     "start_time": "2020-09-28T16:38:06.826643Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = Model(tf_model.input, tf_model.get_layer(index=2).output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T13:09:36.880210Z",
     "start_time": "2020-09-28T13:09:36.874032Z"
    }
   },
   "source": [
    "Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:07.027478Z",
     "start_time": "2020-09-28T16:38:06.831325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"437pt\" height=\"215pt\" viewBox=\"0.00 0.00 449.00 221.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(.9722 .9722) rotate(0) translate(4 217)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-217 445,-217 445,4 -4,4\"/>\n",
       "<!-- 140512038627744 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140512038627744</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"86,-166.5 86,-212.5 355,-212.5 355,-166.5 86,-166.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"148.5\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"211,-166.5 211,-212.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"238.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"211,-189.5 266,-189.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"238.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"266,-166.5 266,-212.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"310.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(None, 1828)</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"266,-189.5 355,-189.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"310.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(None, 1828)</text>\n",
       "</g>\n",
       "<!-- 140512038626688 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140512038626688</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"54,-83.5 54,-129.5 387,-129.5 387,-83.5 54,-83.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"134.5\" y=\"-102.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"215,-83.5 215,-129.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"242.5\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"215,-106.5 270,-106.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"242.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"270,-83.5 270,-129.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"328.5\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(None, 1828)</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"270,-106.5 387,-106.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"328.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(None, 1828, 200)</text>\n",
       "</g>\n",
       "<!-- 140512038627744&#45;&gt;140512038626688 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140512038627744-&gt;140512038626688</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M220.5,-166.3799C220.5,-158.1745 220.5,-148.7679 220.5,-139.8786\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"224.0001,-139.784 220.5,-129.784 217.0001,-139.784 224.0001,-139.784\"/>\n",
       "</g>\n",
       "<!-- 140517372151216 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140517372151216</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"0,-.5 0,-46.5 441,-46.5 441,-.5 0,-.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"134.5\" y=\"-19.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">bidirectional_1(lstm_1): Bidirectional(LSTM)</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"269,-.5 269,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"296.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">input:</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"269,-23.5 324,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"296.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">output:</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"324,-.5 324,-46.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"382.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(None, 1828, 200)</text>\n",
       "<polyline fill=\"none\" stroke=\"#000000\" points=\"324,-23.5 441,-23.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"382.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">(None, 200)</text>\n",
       "</g>\n",
       "<!-- 140512038626688&#45;&gt;140517372151216 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140512038626688-&gt;140517372151216</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M220.5,-83.3799C220.5,-75.1745 220.5,-65.7679 220.5,-56.8786\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"224.0001,-56.784 220.5,-46.784 217.0001,-56.784 224.0001,-56.784\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(embedding_model, show_shapes=True, dpi=70).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve embeddings using the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:07.641078Z",
     "start_time": "2020-09-28T16:38:07.637666Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:08.279993Z",
     "start_time": "2020-09-28T16:38:08.193943Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_df = pd.read_csv(word_index_filepath, index_col='term')\n",
    "word_to_idx_map = word_index_df.to_dict()['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:08.499559Z",
     "start_time": "2020-09-28T16:38:08.492555Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokens_to_sequence(tokens, word_to_idx_map, maxlen):\n",
    "    sequence = []\n",
    "    for token in tokens:\n",
    "        if token in word_to_idx_map:\n",
    "            token_idx = word_to_idx_map[token]\n",
    "            sequence.append(token_idx)\n",
    "    \n",
    "    padded_sequence = pad_sequences([sequence], maxlen=maxlen).reshape(-1)\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get model input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:09.322723Z",
     "start_time": "2020-09-28T16:38:09.317238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1828"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = tf_model.input.shape[1]\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:10.125248Z",
     "start_time": "2020-09-28T16:38:10.121887Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_sequences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:11.364330Z",
     "start_time": "2020-09-28T16:38:10.429430Z"
    }
   },
   "outputs": [],
   "source": [
    "for doc_id in reuters_corpus.docs:\n",
    "    doc_tokens = reuters_corpus.get_tokens(doc_id)\n",
    "    doc_sequence = tokens_to_sequence(doc_tokens, word_to_idx_map, maxlen)\n",
    "    doc_sequences.append(doc_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:38:11.407177Z",
     "start_time": "2020-09-28T16:38:11.366479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9848, 1828)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sequences_np = np.array(doc_sequences)\n",
    "doc_sequences_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:40:38.780616Z",
     "start_time": "2020-09-28T16:38:11.408986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9848, 200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = embedding_model.predict(doc_sequences_np)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample documents using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:42:59.812319Z",
     "start_time": "2020-09-28T16:40:38.783218Z"
    }
   },
   "outputs": [],
   "source": [
    "label_to_data_idx_map = nn_model.label_to_data_idx(reuters_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:43:21.004455Z",
     "start_time": "2020-09-28T16:43:20.660108Z"
    }
   },
   "outputs": [],
   "source": [
    "from sample import KMeansSampler\n",
    "\n",
    "sampler = KMeansSampler(reuters_corpus, \n",
    "                        embedding_matrix,\n",
    "                        label_to_data_idx_map,\n",
    "                        min_size=500,\n",
    "                        max_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function used for sampling data for each concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:43:23.221581Z",
     "start_time": "2020-09-28T16:43:23.216948Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_data_mp(sampler, label_idx):\n",
    "    sample_data_idxs = sampler.sample_data(label_idx)\n",
    "    return (label_idx, sample_data_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of available CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:43:24.265700Z",
     "start_time": "2020-09-28T16:43:24.262002Z"
    }
   },
   "outputs": [],
   "source": [
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:43:24.675458Z",
     "start_time": "2020-09-28T16:43:24.663157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psutil.cpu_count(logical=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of parallel job for the sampling step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:43:26.608158Z",
     "start_time": "2020-09-28T16:43:26.604385Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_jobs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:43:27.958979Z",
     "start_time": "2020-09-28T16:43:27.955765Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:44:37.075992Z",
     "start_time": "2020-09-28T16:43:28.619943Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of  20 | elapsed:   12.5s remaining:   18.7s\n",
      "[Parallel(n_jobs=8)]: Done  11 out of  20 | elapsed:   16.3s remaining:   13.4s\n",
      "[Parallel(n_jobs=8)]: Done  14 out of  20 | elapsed:   17.9s remaining:    7.7s\n",
      "[Parallel(n_jobs=8)]: Done  17 out of  20 | elapsed:   20.1s remaining:    3.6s\n",
      "[Parallel(n_jobs=8)]: Done  20 out of  20 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  20 out of  20 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "label_to_sample_idxs = Parallel(n_jobs=kmeans_jobs, verbose=10)(delayed(sample_data_mp)(sampler, label_idx) for label_idx in label_to_data_idx_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Find relevant terms for each cluster label using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:44:57.500624Z",
     "start_time": "2020-09-28T16:44:57.490026Z"
    }
   },
   "outputs": [],
   "source": [
    "from termfinder import LimeTermFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:44:58.305971Z",
     "start_time": "2020-09-28T16:44:58.294436Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_relevant_terms_mp(model, corpus, input_data):\n",
    "    result_list = []\n",
    "    \n",
    "    term_finder = LimeTermFinder(model, corpus)\n",
    "    \n",
    "    for label_idx, data_idx in input_data:\n",
    "        relevant_terms = term_finder.get_relevant_terms(data_idx, label_idx)\n",
    "        \n",
    "        if relevant_terms:\n",
    "            \n",
    "            for term, weight in relevant_terms.items():\n",
    "                dict_entry = {'label': corpus.labels[label_idx],\n",
    "                              'term': term,\n",
    "                              'weight': weight,\n",
    "                              'data_id': corpus.docs[data_idx]}\n",
    "                result_list.append(dict_entry)\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into multiple batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:45:00.093548Z",
     "start_time": "2020-09-28T16:45:00.090415Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice input data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:45:01.884989Z",
     "start_time": "2020-09-28T16:45:01.870875Z"
    }
   },
   "outputs": [],
   "source": [
    "input_data = np.array([(label_idx, data_idx) for label_idx, data_idxs in label_to_sample_idxs for data_idx in data_idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of parallel jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:45:05.210370Z",
     "start_time": "2020-09-28T16:45:05.207098Z"
    }
   },
   "outputs": [],
   "source": [
    "lime_jobs = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T16:45:06.285000Z",
     "start_time": "2020-09-28T16:45:06.280832Z"
    }
   },
   "outputs": [],
   "source": [
    "input_slices = np.array_split(input_data, lime_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, find relevant terms using `LIME`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T18:04:17.361511Z",
     "start_time": "2020-09-28T16:45:08.358439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Using backend LokyBackend with 14 concurrent workers.\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "[Parallel(n_jobs=14)]: Done   3 out of  14 | elapsed: 1225.8min remaining: 4494.8min\n",
      "[Parallel(n_jobs=14)]: Done   5 out of  14 | elapsed: 1227.5min remaining: 2209.5min\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "[Parallel(n_jobs=14)]: Done   7 out of  14 | elapsed: 1239.0min remaining: 1239.0min\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "[Parallel(n_jobs=14)]: Done   9 out of  14 | elapsed: 1251.5min remaining: 695.3min\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "[Parallel(n_jobs=14)]: Done  11 out of  14 | elapsed: 1484.4min remaining: 404.8min\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Exception ignored in: <Finalize object, dead>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/multiprocessing/util.py\", line 201, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/nvidia/anaconda3/envs/testenv/lib/python3.8/site-packages/joblib/externals/loky/backend/synchronize.py\", line 96, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "[Parallel(n_jobs=14)]: Done  14 out of  14 | elapsed: 1519.1min finished\n"
     ]
    }
   ],
   "source": [
    "terms_list_tmp = Parallel(n_jobs=lime_jobs, verbose=10, batch_size=1)(delayed(get_relevant_terms_mp)(nn_model, reuters_corpus, input_batch) for input_batch in input_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a DataFrame out of `terms_list_tmp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T19:05:25.978968Z",
     "start_time": "2020-09-29T19:05:25.971231Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data = []\n",
    "for sublist in terms_list_tmp:\n",
    "    df_data += sublist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T19:05:27.326751Z",
     "start_time": "2020-09-29T19:05:27.320409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'earn',\n",
       "  'term': '15_cts',\n",
       "  'weight': 0.039050247438999125,\n",
       "  'data_id': 3405},\n",
       " {'label': 'earn',\n",
       "  'term': 'div',\n",
       "  'weight': 0.03501927337817323,\n",
       "  'data_id': 3405},\n",
       " {'label': 'earn',\n",
       "  'term': 'payout',\n",
       "  'weight': 0.034544346395799584,\n",
       "  'data_id': 3405}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_terms_df = pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_terms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Save retrieved terms to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_dir = 'terms'\n",
    "filename = 'relevant_terms_reuters_nn.csv'\n",
    "filepath = os.path.join(root_dir, data_dir, terms_dir, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_terms_df.to_csv(filepath, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Check for pending joblib processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import active_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
